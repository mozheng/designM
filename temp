import torch
import sys
import argparse
import numpy as np
from pathlib import Path
from matplotlib import pyplot as plt
from PIL import Image
import imageio
import cv2
from sam_segment import predict_masks_with_sam
from lama_inpaint import inpaint_img_with_lama
from utils import load_img_to_array, save_array_to_img, dilate_mask, \
    show_mask, show_points, get_clicked_point


def setup_args(parser):
    parser.add_argument(
        "--input_img", type=str, required=True,
        help="Path to a single input img",
    )
    parser.add_argument(
        "--coords_type", type=str, required=True,
        default="key_in", choices=["click", "key_in"], 
        help="The way to select coords",
    )
    parser.add_argument(
        "--point_coords", type=float, nargs='+', required=True,
        help="The coordinate of the point prompt, [coord_W coord_H].",
    )
    parser.add_argument(
        "--point_labels", type=int, nargs='+', required=True,
        help="The labels of the point prompt, 1 or 0.",
    )
    parser.add_argument(
        "--dilate_kernel_size", type=int, default=None,
        help="Dilate kernel size. Default: None",
    )
    parser.add_argument(
        "--output_dir", type=str, required=True,
        help="Output path to the directory with results.",
    )
    parser.add_argument(
        "--sam_model_type", type=str,
        default="vit_h", choices=['vit_h', 'vit_l', 'vit_b'],
        help="The type of sam model to load. Default: 'vit_h"
    )
    parser.add_argument(
        "--sam_ckpt", type=str, required=True,
        help="The path to the SAM checkpoint to use for mask generation.",
    )
    parser.add_argument(
        "--lama_config", type=str,
        default="./lama/configs/prediction/default.yaml",
        help="The path to the config file of lama model. "
             "Default: the config of big-lama",
    )
    parser.add_argument(
        "--lama_ckpt", type=str, required=True,
        help="The path to the lama checkpoint.",
    )



def rotate_img(img, angle, points=None):
    '''
    img   --image
    angle --rotation angle
    return--rotated img
    '''
    h, w = img.shape[:2]
    if points is None:
        rotate_center = (w/2, h/2)
    else:
        rotate_center = points
    #获取旋转矩阵
    # 参数1为旋转中心点;
    # 参数2为旋转角度,正值-逆时针旋转;负值-顺时针旋转
    # 参数3为各向同性的比例因子,1.0原图，2.0变成原来的2倍，0.5变成原来的0.5倍
    M = cv2.getRotationMatrix2D(rotate_center, angle, 1.0)
    #计算图像新边界
    new_w = int(h * np.abs(M[0, 1]) + w * np.abs(M[0, 0]))
    new_h = int(h * np.abs(M[0, 0]) + w * np.abs(M[0, 1]))
    #调整旋转矩阵以考虑平移
    M[0, 2] += (new_w - w) / 2
    M[1, 2] += (new_h - h) / 2

    # rotated_img = cv2.warpAffine(img, M, (new_w, new_h))
    rotated_img = cv2.warpAffine(img, M, (w, h))
    return rotated_img

def image_add(image1, image2, mask):
    back_r,back_g,back_b = cv2.split(image1)
    mask_inverse = cv2.bitwise_not(mask)
    r = cv2.bitwise_and(back_r, mask_inverse)
    g = cv2.bitwise_and(back_g, mask_inverse)
    b = cv2.bitwise_and(back_b, mask_inverse)
    back =  np.stack((r,g,b), axis=2)

    return cv2.addWeighted(back,1,image2,1,0)



if __name__ == "__main__":
    """Example usage:
    python remove_anything.py \
        --input_img FA_demo/FA1_dog.png \
        --coords_type key_in \
        --point_coords 750 500 \
        --point_labels 1 \
        --dilate_kernel_size 15 \
        --output_dir ./results \
        --sam_model_type "vit_h" \
        --sam_ckpt sam_vit_h_4b8939.pth \
        --lama_config lama/configs/prediction/default.yaml \
        --lama_ckpt big-lama 
    """
    parser = argparse.ArgumentParser()
    setup_args(parser)
    args = parser.parse_args(sys.argv[1:])
    device = "cuda" if torch.cuda.is_available() else "cpu"

    if args.coords_type == "click":
        latest_coords = get_clicked_point(args.input_img)
    elif args.coords_type == "key_in":
        latest_coords = args.point_coords
    img = load_img_to_array(args.input_img)

    masks, _, _ = predict_masks_with_sam(
        img,
        [latest_coords],
        args.point_labels,
        model_type=args.sam_model_type,
        ckpt_p=args.sam_ckpt,
        device=device,
    )
    raw_masks = masks.astype(np.uint8) * 255

    # dilate mask to avoid unmasked edge effect 要做
    if args.dilate_kernel_size is not None:
        masks = [dilate_mask(mask, args.dilate_kernel_size) for mask in raw_masks]
    else:
        masks = raw_masks

    # visualize the segmentation results
    img_stem = Path(args.input_img).stem
    out_dir = Path(args.output_dir) / img_stem
    out_dir.mkdir(parents=True, exist_ok=True)

    r,g,b = cv2.split(img) 

    for idx, (rmask,mask) in enumerate(zip(raw_masks, masks)):
        # path to the results
        mask_p = out_dir / f"mask_{idx}.png"

        # save the mask        
        r_and = cv2.bitwise_and(r, rmask)
        g_and = cv2.bitwise_and(g, rmask)
        b_and = cv2.bitwise_and(b, rmask)
        img_and = np.stack((r_and, g_and, b_and), axis=2)
        save_array_to_img(img_and, mask_p)

        # inpaint the masked image
        img_inpainted_p = out_dir / f"inpainted_with_{Path(mask_p).name}"
        img_inpainted = inpaint_img_with_lama(
            img, mask, args.lama_config, args.lama_ckpt, device=device)
        save_array_to_img(img_inpainted, img_inpainted_p)

        print(img_and.shape, img.shape)
        front = rotate_img(img_and, -10, latest_coords)
        frontmask = rotate_img(rmask, -10, latest_coords)
        print(front.shape, img_inpainted.shape)
        frame1 = image_add(img_inpainted, front, frontmask)

        front = rotate_img(img_and, 10, latest_coords)
        frontmask = rotate_img(rmask, 10, latest_coords)
        frame2 = image_add(img_inpainted, front, frontmask)

        gif_images=[frame1, frame2]
        img_gif = out_dir / f"test_{idx}.gif"
        imageio.mimsave(img_gif,gif_images,fps=2)        

